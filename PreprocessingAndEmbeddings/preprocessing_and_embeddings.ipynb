{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zb33YWO3y0pS"
   },
   "source": [
    "# Preprocessing and Embeddings\n",
    "\n",
    "We will explore preprocessing of word representations using the package nltk.\n",
    "\n",
    "Note that other NLP tools, such as [SpaCy](https://spacy.io/), or [Stanza](https://stanfordnlp.github.io/stanza/) are popular alternatives which provide higher levels of abstractions than NLTK. When working on an NLP task, the use of one of those two libraries is recommended over NLTK.\n",
    "\n",
    "The code below will run through implementing a Word2Vec algorithm from scratch. A fuller and wholesome tutorial can be found in the \"DeeperDiveIntoWordEmbeddings.zip\" folder. Feel free to take a read through the notebook there if you want more information, or if you found yourself struggling to understand every concept from the taught theory.\n",
    "\n",
    "Finally, we will evaluate word representations from pre-trained word embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2869,
     "status": "ok",
     "timestamp": 1737550828306,
     "user": {
      "displayName": "Eder Tarifa Fernandez",
      "userId": "12042907008888851310"
     },
     "user_tz": 0
    },
    "id": "5qS8y5_Ewv6n",
    "outputId": "64bb9119-fce6-439c-8f7a-653c0b7971a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FE8B9-L8U0aZ"
   },
   "source": [
    "# Semantic word representations\n",
    "\n",
    "\n",
    "Recall that *semantic Word Representations* are representations that are learned to capture the 'meaning' of a word. These are low-dimensional vectors that contain some semantic properties. In this notebook we are going to build state-of-the art approaches to obtain semantic word representations using the **word2vec** modelling approach. We will also use these vectors in some  tasks to understand the utility of these representations.\n",
    "\n",
    "We begin by loading some of the libraries that are necessary for building our model. We are using [pytorch](https://pytorch.org/), an open source deep learning platform, as our backbone library in the course.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1737550828306,
     "user": {
      "displayName": "Eder Tarifa Fernandez",
      "userId": "12042907008888851310"
     },
     "user_tz": 0
    },
    "id": "XG82efAzvcbK"
   },
   "outputs": [],
   "source": [
    "#@title Loading packages\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from scipy.spatial.distance import euclidean, cosine\n",
    "from tqdm import tqdm\n",
    "import codecs\n",
    "from sklearn.metrics.pairwise import cosine_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1737550828306,
     "user": {
      "displayName": "Eder Tarifa Fernandez",
      "userId": "12042907008888851310"
     },
     "user_tz": 0
    },
    "id": "abMSnMwJx8bu"
   },
   "outputs": [],
   "source": [
    "#@title Sample corpora\n",
    "\n",
    "corpus = [\n",
    "    'he is a king',\n",
    "    'she is a queen',\n",
    "    'he is a Man',\n",
    "    'she Is a woman',\n",
    "    'london is, the capital of England',\n",
    "    'Berlin is ... the capital of germany',\n",
    "    'paris is the capital of france.',\n",
    "    'He will eat cake, pie, and/or brownies',\n",
    "    \"she didn't like the brownies\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0hPlpqVFYpfL"
   },
   "source": [
    "# Tokenization\n",
    "\n",
    "Q: What is a token and why do we need to tokenize?\n",
    "\n",
    "Q: Print the tokenized corpus above. What mistakes do you find in the code below?\n",
    "\n",
    "Q: What could be a nice way of fixing these mistakes?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1737550828306,
     "user": {
      "displayName": "Eder Tarifa Fernandez",
      "userId": "12042907008888851310"
     },
     "user_tz": 0
    },
    "id": "UQ5IL1e1GVn6",
    "outputId": "21c85639-b1a7-4293-9d21-d02d31c284ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['he', 'is', 'a', 'king'], ['she', 'is', 'a', 'queen'], ['he', 'is', 'a', 'Man'], ['she', 'Is', 'a', 'woman'], ['london', 'is', 'the', 'capital', 'of', 'England'], ['Berlin', 'is', 'the', 'capital', 'of', 'germany'], ['paris', 'is', 'the', 'capital', 'of', 'france.'], ['He', 'will', 'eat', 'cake', 'pie', 'and/or', 'brownies'], ['she', \"didn't\", 'like', 'the', 'brownies']]\n"
     ]
    }
   ],
   "source": [
    "tokenized_corpus = [] # Let us put the tokenized corpus in a list\n",
    "for sentence in corpus:\n",
    "  tokenized_sentence = []\n",
    "  for token in sentence.split(' '): # simplest split is\n",
    "    lastletter = token[-1:]\n",
    "    stopwords = [\"...\"]\n",
    "    if token in stopwords:\n",
    "      pass\n",
    "    elif lastletter == \",\":\n",
    "      token = token[:-1]\n",
    "      tokenized_sentence.append(token)\n",
    "    else:\n",
    "      tokenized_sentence.append(token)\n",
    "  tokenized_corpus.append(tokenized_sentence)\n",
    "\n",
    "print(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NPWECLTEmAG9"
   },
   "source": [
    "# Pre-processing\n",
    "\n",
    "Tokenization is a crucial pre-processing step in the NLP domain`*`. However, other pre-processing techniques also exist, many of which were extensively employed in rule-based and statistical NLP. While we don't utilise these pre-processing techniques in neural-based NLP anymore, they are still worth a recap. Typically, **stop words** and **punctuation** removal are employed, along with *either* **stemming** or **lemmatization**. However, in the following code, we will demonstrate each of the techniques separately (mainly due to our corpus being so small)\n",
    "\n",
    "### Stop Word removal\n",
    "Stop words are generally the most common words in the language which who's meaning in a sequenece is ambiguous. Some examples of stop words are: The, a, an, that.\n",
    "\n",
    "### Punctuation removal\n",
    "Old school NLP techniques (and some modern day ones) struggle to understand the semantics of punctuation. Thus, they were also removed.\n",
    "\n",
    "## Stemming and Lemmatization\n",
    "Stemming and Lemmatization are two distinct word normalization techniques. Essentially this means that, given our corpora, we wish to have variants of a word in a 'normal' form. For example, [playing, plays, played] may be normalised to \"Play\". The sentence \"the boy's cars are different colours\" may be normalised to \"the boy car be differ colour\"\n",
    "\n",
    "### Stemming\n",
    "In the case of stemming, we want to normalise all words to their stem (or root). The stem is the part of the word to which affixes (suffixes or prefixes) are assigned. Stemming a word may result in the word not actually being a word. For example, some stemming algorithms may stem [trouble, troubling, troubled] as \"troubl\".\n",
    "\n",
    "### Lemmatization\n",
    "Lemmatization attempts to properly reduce unnormalized tokens to a word that belongs in the language. The root word is called a **lemma**, and is the canonical form of a set of words. For example, [runs, running, ran] are all forms of the word \"run.\n",
    "\n",
    "\n",
    "\n",
    "Q. Think of two or three other stop words, and add them to the list of stop words below.\n",
    "\n",
    "Q. Using the examples provided below, write some code which both removes stop words and punctuation from our corpus\n",
    "\n",
    "Q. The examples of stemming and lemmatization below are on words/sequences not in our corpus. Extend the code so it works on our corpus.\n",
    "\n",
    "\n",
    "N.B. We are not going to use these techniques in this file after this section, so we will demonstrate how to perform these techniques distinctly on our toy corpus.\n",
    "\n",
    "`*`Recently there has been newer approaches to \"tokenization\" which goes further than one token being one word. One example is [SentencePiece](https://github.com/google/sentencepiece). These approaches are out of scope for this lab session, but may appear in future sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9015,
     "status": "ok",
     "timestamp": 1737550837318,
     "user": {
      "displayName": "Eder Tarifa Fernandez",
      "userId": "12042907008888851310"
     },
     "user_tz": 0
    },
    "id": "k_DGoHbPmAG-",
    "outputId": "81b67ca1-fd20-448b-a7c0-b3d885ce0a99"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt') # Download the tokenizer model\n",
    "nltk.download('wordnet') # Download the wordnet corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 210,
     "status": "ok",
     "timestamp": 1737550837525,
     "user": {
      "displayName": "Eder Tarifa Fernandez",
      "userId": "12042907008888851310"
     },
     "user_tz": 0
    },
    "id": "FuSjoUpfmAHA",
    "outputId": "f38a6359-453a-48cc-b9a0-ad843a37f819"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['he', 'is', 'king'], ['she', 'is', 'queen'], ['he', 'is', 'Man'], ['she', 'Is', 'woman'], ['london', 'is,', 'capital', 'England'], ['Berlin', 'is', 'capital', 'germany'], ['paris', 'is', 'capital', 'france.'], ['He', 'will', 'eat', 'cake,', 'pie,', 'and/or', 'brownies'], ['she', \"didn't\", 'like', 'brownies']]\n"
     ]
    }
   ],
   "source": [
    "# STOP WORD REMOVAL\n",
    "stop_words_list = [\"the\", \"a\", \"an\", \"that\", \"or\", \"of\", \"and\", \"...\"] # Add stop words from Q1 here.\n",
    "punctuation = [\",\", \".\", \")\", \"(\", \":\", \";\"]\n",
    "# SWR = stop words removed\n",
    "tokenized_corpus_SWR = []\n",
    "for sentence in corpus:\n",
    "    tokenized_sentence_SWR = []\n",
    "    for token in sentence.split(\" \"):\n",
    "        lastletter = token[-1:]\n",
    "        if lastletter in punctuation and lastletter not in punctuation:\n",
    "          token = token[:-1]\n",
    "          tokenized_sentence_SWR.append(token)\n",
    "        elif token not in stop_words_list:\n",
    "            tokenized_sentence_SWR.append(token)\n",
    "\n",
    "    if tokenized_sentence_SWR: # Only append to corpus if tokenized_sentence_SWR isn't empty\n",
    "        tokenized_corpus_SWR.append(tokenized_sentence_SWR)\n",
    "\n",
    "print(tokenized_corpus_SWR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1737550837525,
     "user": {
      "displayName": "Eder Tarifa Fernandez",
      "userId": "12042907008888851310"
     },
     "user_tz": 0
    },
    "id": "3VtK8AQ_mAHC",
    "outputId": "31b4a0a1-b3fb-4b76-819f-b27747fd4a91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['he', 'is', 'a', 'king'], ['she', 'is', 'a', 'queen'], ['he', 'is', 'a', 'Man'], ['she', 'Is', 'a', 'woman'], ['london', 'is', 'the', 'capital', 'of', 'England'], ['Berlin', 'is', 'the', 'capital', 'of', 'germany'], ['paris', 'is', 'the', 'capital', 'of', 'france'], ['He', 'will', 'eat', 'cake', 'pie', 'and', 'or', 'brownies'], ['she', 'didn', 't', 'like', 'the', 'brownies']]\n"
     ]
    }
   ],
   "source": [
    "# PUNCTUATION REMOVAL\n",
    "import re # regex\n",
    "\n",
    "re_punctuation_string = '[\\s,/.\\']'\n",
    "\n",
    "# PR = punctuation removed\n",
    "tokenized_corpus_PR = []\n",
    "for sentence in corpus:\n",
    "    tokenized_sentence_PR = re.split(re_punctuation_string, sentence) # in python's regex, [...] is an alternative to writing .|.|.\n",
    "    tokenized_sentence_PR = list(filter(None, tokenized_sentence_PR)) # remove empty strings from list\n",
    "    tokenized_corpus_PR.append(tokenized_sentence_PR)\n",
    "\n",
    "print(tokenized_corpus_PR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1737550837526,
     "user": {
      "displayName": "Eder Tarifa Fernandez",
      "userId": "12042907008888851310"
     },
     "user_tz": 0
    },
    "id": "YyzTW_wXmAHF"
   },
   "outputs": [],
   "source": [
    "# ANSWER Q2 HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1737550837526,
     "user": {
      "displayName": "Eder Tarifa Fernandez",
      "userId": "12042907008888851310"
     },
     "user_tz": 0
    },
    "id": "NZGX31WamAHH",
    "outputId": "e178179b-9c7c-4904-b312-eb383c500ec8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Stemmed variant     \n",
      "\n",
      "friend              friend              \n",
      "friendship          friendship          \n",
      "friends             friend              \n",
      "friendships         friendship          \n",
      "stabil              stabil              \n",
      "destabilize         destabil            \n",
      "misunderstanding    misunderstand       \n",
      "railroad            railroad            \n",
      "moonlight           moonlight           \n",
      "football            footbal             \n"
     ]
    }
   ],
   "source": [
    "# STEMMING\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "stemming_word_list = [\"friend\", \"friendship\", \"friends\", \"friendships\",\"stabil\",\"destabilize\",\"misunderstanding\",\"railroad\",\"moonlight\",\"football\"]\n",
    "print(\"{0:20}{1:20}\".format(\"Word\",\"Stemmed variant\"))\n",
    "print()\n",
    "\n",
    "for word in stemming_word_list:\n",
    "      print(\"{0:20}{1:20}\".format(word,porter.stem(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11267,
     "status": "ok",
     "timestamp": 1737550848790,
     "user": {
      "displayName": "Eder Tarifa Fernandez",
      "userId": "12042907008888851310"
     },
     "user_tz": 0
    },
    "id": "Muvs2GZ4mAHK",
    "outputId": "a39848dd-d308-4850-ef91-c61f4d1a8083"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Lemma               \n",
      "\n",
      "He                  He                  \n",
      "was                 wa                  \n",
      "running             running             \n",
      "and                 and                 \n",
      "eating              eating              \n",
      "at                  at                  \n",
      "same                same                \n",
      "time                time                \n",
      "He                  He                  \n",
      "has                 ha                  \n",
      "bad                 bad                 \n",
      "habit               habit               \n",
      "of                  of                  \n",
      "swimming            swimming            \n",
      "after               after               \n",
      "playing             playing             \n",
      "long                long                \n",
      "hours               hour                \n",
      "in                  in                  \n",
      "the                 the                 \n",
      "Sun                 Sun                 \n"
     ]
    }
   ],
   "source": [
    "# LEMMATIZATION\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "to_lemmatize_sentence = \"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\n",
    "\n",
    "# lemmatization requires punctuation removal\n",
    "to_lemmatize_sentence = re.split(re_punctuation_string, to_lemmatize_sentence)\n",
    "to_lemmatize_sentence = list(filter(None, to_lemmatize_sentence))\n",
    "\n",
    "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
    "print()\n",
    "\n",
    "for word in to_lemmatize_sentence:\n",
    "    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1737550848790,
     "user": {
      "displayName": "Eder Tarifa Fernandez",
      "userId": "12042907008888851310"
     },
     "user_tz": 0
    },
    "id": "CL6qkUJPmAHM",
    "outputId": "934c504c-8dac-47a4-c19b-255c1c5b704b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Lemma               \n",
      "\n",
      "He                  He                  \n",
      "was                 be                  \n",
      "running             run                 \n",
      "and                 and                 \n",
      "eating              eat                 \n",
      "at                  at                  \n",
      "same                same                \n",
      "time                time                \n",
      "He                  He                  \n",
      "has                 have                \n",
      "bad                 bad                 \n",
      "habit               habit               \n",
      "of                  of                  \n",
      "swimming            swim                \n",
      "after               after               \n",
      "playing             play                \n",
      "long                long                \n",
      "hours               hours               \n",
      "in                  in                  \n",
      "the                 the                 \n",
      "Sun                 Sun                 \n"
     ]
    }
   ],
   "source": [
    "# Why didn't the above do anything?\n",
    "# It's because the lemmatizer requires parts of speech (POS) context about the word it is currently parsing.\n",
    "# We would need to use a POS model to identify what the POS for a token in its context is.\n",
    "# In the above example (and for Q3), we'll just pass in the VERB context for every token\n",
    "\n",
    "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
    "print()\n",
    "\n",
    "for word in to_lemmatize_sentence:\n",
    "    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word, pos=\"v\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1737550848790,
     "user": {
      "displayName": "Eder Tarifa Fernandez",
      "userId": "12042907008888851310"
     },
     "user_tz": 0
    },
    "id": "qXfoaPJ3mAHP",
    "outputId": "caea0e9c-eeb1-4d7b-cb4e-4031ab50ef7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Stemmed variant     \n",
      "\n",
      "he                  he                  \n",
      "is                  is                  \n",
      "a                   a                   \n",
      "king                king                \n",
      "she                 she                 \n",
      "is                  is                  \n",
      "a                   a                   \n",
      "queen               queen               \n",
      "he                  he                  \n",
      "is                  is                  \n",
      "a                   a                   \n",
      "Man                 man                 \n",
      "she                 she                 \n",
      "Is                  is                  \n",
      "a                   a                   \n",
      "woman               woman               \n",
      "london              london              \n",
      "is                  is                  \n",
      "the                 the                 \n",
      "capital             capit               \n",
      "of                  of                  \n",
      "England             england             \n",
      "Berlin              berlin              \n",
      "is                  is                  \n",
      "the                 the                 \n",
      "capital             capit               \n",
      "of                  of                  \n",
      "germany             germani             \n",
      "paris               pari                \n",
      "is                  is                  \n",
      "the                 the                 \n",
      "capital             capit               \n",
      "of                  of                  \n",
      "france.             france.             \n",
      "He                  he                  \n",
      "will                will                \n",
      "eat                 eat                 \n",
      "cake                cake                \n",
      "pie                 pie                 \n",
      "and/or              and/or              \n",
      "brownies            browni              \n",
      "she                 she                 \n",
      "didn't              didn't              \n",
      "like                like                \n",
      "the                 the                 \n",
      "brownies            browni              \n",
      "---------------------------\n",
      "Word                Lemma               \n",
      "\n",
      "he                  he                  \n",
      "is                  be                  \n",
      "a                   a                   \n",
      "king                king                \n",
      "she                 she                 \n",
      "is                  be                  \n",
      "a                   a                   \n",
      "queen               queen               \n",
      "he                  he                  \n",
      "is                  be                  \n",
      "a                   a                   \n",
      "Man                 Man                 \n",
      "she                 she                 \n",
      "Is                  Is                  \n",
      "a                   a                   \n",
      "woman               woman               \n",
      "london              london              \n",
      "is                  be                  \n",
      "                                        \n",
      "the                 the                 \n",
      "capital             capital             \n",
      "of                  of                  \n",
      "England             England             \n",
      "Berlin              Berlin              \n",
      "is                  be                  \n",
      "                                        \n",
      "                                        \n",
      "                                        \n",
      "                                        \n",
      "the                 the                 \n",
      "capital             capital             \n",
      "of                  of                  \n",
      "germany             germany             \n",
      "paris               paris               \n",
      "is                  be                  \n",
      "the                 the                 \n",
      "capital             capital             \n",
      "of                  of                  \n",
      "france              france              \n",
      "                                        \n",
      "He                  He                  \n",
      "will                will                \n",
      "eat                 eat                 \n",
      "cake                cake                \n",
      "                                        \n",
      "pie                 pie                 \n",
      "                                        \n",
      "and                 and                 \n",
      "or                  or                  \n",
      "brownies            brownies            \n",
      "she                 she                 \n",
      "didn                didn                \n",
      "t                   t                   \n",
      "like                like                \n",
      "the                 the                 \n",
      "brownies            brownies            \n"
     ]
    }
   ],
   "source": [
    "# ANSWER Q3 HERE (make sure not to overwrite our \"tokenized_corpus\" variable from the \"\")\n",
    "\n",
    "re_punctuation_string = '[\\s,/.\\']'\n",
    "\n",
    "# For Stemming -- Use \"tokenized_corpus\".\n",
    "print(\"{0:20}{1:20}\".format(\"Word\",\"Stemmed variant\"))\n",
    "print()\n",
    "\n",
    "for line in tokenized_corpus:\n",
    "  for word in line:\n",
    "      print(\"{0:20}{1:20}\".format(word,porter.stem(word)))\n",
    "# For Lemmatization -- Remove punctuation first (See example above).\n",
    "print(\"---------------------------\")\n",
    "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
    "print()\n",
    "for line in corpus:\n",
    "  to_lemmatize_sentence = re.split(re_punctuation_string, line)\n",
    "  #to_lemmatize_sentence = list(filter(None, line)) Con esto te lo pone en letras????\n",
    "\n",
    "  for word in to_lemmatize_sentence:\n",
    "    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word, pos=\"v\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cyKCfY6QbO0r"
   },
   "source": [
    "# Vocabulary\n",
    "\n",
    "The code below obtains the vocabulary of the corpus.\n",
    "\n",
    "Q. Print the size of the vocabulary.\n",
    "\n",
    "Q. A programatically cleaner (and shorter) way of writing the code below by using a set instead of a list. Can you implement the code below using a set?\n",
    "\n",
    "##### 3 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1737550848790,
     "user": {
      "displayName": "Eder Tarifa Fernandez",
      "userId": "12042907008888851310"
     },
     "user_tz": 0
    },
    "id": "9bWQ3zKYKlQU",
    "outputId": "c595d905-40b1-447e-e176-ed9233cb2b57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['he', 'is', 'a', 'king', 'she', 'queen', 'Man', 'Is', 'woman', 'london', 'the', 'capital', 'of', 'England', 'Berlin', 'germany', 'paris', 'france.', 'He', 'will', 'eat', 'cake', 'pie', 'and/or', 'brownies', \"didn't\", 'like']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = [] # Let us put all the tokens (mostly words)\n",
    "                # appearing in the vocabulary in a list\n",
    "\n",
    "for sentence in tokenized_corpus:\n",
    "    for token in sentence:\n",
    "        if token not in vocabulary:\n",
    "            vocabulary.append(token)\n",
    "\n",
    "\n",
    "print(vocabulary)\n",
    "\n",
    "\n",
    "\n",
    "# Q. what is the size of the vocabulary?\n",
    "\n",
    "vocabulary_size = len(vocabulary)\n",
    "vocabulary_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xGg6CtALe8Va"
   },
   "source": [
    "# Helper functions\n",
    "\n",
    "* These are some of the common helper functions that are used for NLP models:\n",
    "\n",
    "    * `word2idx`:  Maintains a dictionary of word and the corresponding index\n",
    "    \n",
    "    * `idx2word`: Maintains a mapping from index to word\n",
    "    \n",
    "    \n",
    "* Print the word2idx and idx2word, we will be using these in future exercises.\n",
    "\n",
    "##### 3 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1737550848790,
     "user": {
      "displayName": "Eder Tarifa Fernandez",
      "userId": "12042907008888851310"
     },
     "user_tz": 0
    },
    "id": "4LWut1gtXGQN",
    "outputId": "5ba5a207-8ab2-4266-879d-529d5e832ead"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'he': 0, 'is': 1, 'a': 2, 'king': 3, 'she': 4, 'queen': 5, 'Man': 6, 'Is': 7, 'woman': 8, 'london': 9, 'the': 10, 'capital': 11, 'of': 12, 'England': 13, 'Berlin': 14, 'germany': 15, 'paris': 16, 'france.': 17, 'He': 18, 'will': 19, 'eat': 20, 'cake': 21, 'pie': 22, 'and/or': 23, 'brownies': 24, \"didn't\": 25, 'like': 26}\n",
      "{0: 'he', 1: 'is', 2: 'a', 3: 'king', 4: 'she', 5: 'queen', 6: 'Man', 7: 'Is', 8: 'woman', 9: 'london', 10: 'the', 11: 'capital', 12: 'of', 13: 'England', 14: 'Berlin', 15: 'germany', 16: 'paris', 17: 'france.', 18: 'He', 19: 'will', 20: 'eat', 21: 'cake', 22: 'pie', 23: 'and/or', 24: 'brownies', 25: \"didn't\", 26: 'like'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "word2idx = {w: idx for (idx, w) in enumerate(vocabulary)}\n",
    "idx2word = {idx: w for (idx, w) in enumerate(vocabulary)}\n",
    "\n",
    "print(word2idx)\n",
    "print(idx2word)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1bFjK86NPChI"
   },
   "source": [
    "# Look-up table\n",
    "\n",
    "* This is a table that maps from an index to a one hot vector.\n",
    "\n",
    "Q. Print one-hot vectors corresponding to the words 'this', 'he' and 'england'\n",
    "\n",
    "##### 3 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 464,
     "status": "ok",
     "timestamp": 1737550849252,
     "user": {
      "displayName": "Eder Tarifa Fernandez",
      "userId": "12042907008888851310"
     },
     "user_tz": 0
    },
    "id": "Vp8OTZI-UrYU",
    "outputId": "150da273-6870-4333-fe47-c5abe3ec9012"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "this :\n",
      "he :\n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "england :\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def look_up_table(word_idx):\n",
    "    x = torch.zeros(vocabulary_size).float()\n",
    "    x[word_idx] = 1.0\n",
    "    return x\n",
    "\n",
    "# This is a one hot representation\n",
    "\n",
    "# See an example for 'is'\n",
    "word_idx = word2idx['is']\n",
    "print(look_up_table(word_idx))\n",
    "\n",
    "# Q: Print one-hot vectors for 'this', 'he', and 'england'\n",
    "words = [\"this\", \"he\", \"england\"]\n",
    "for word in words:\n",
    "  try:\n",
    "    print(word, \":\")\n",
    "    print(look_up_table(word2idx[word]))\n",
    "  except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PcAI5BtaQMFh"
   },
   "source": [
    "# Extracting contexts and the focus word\n",
    "\n",
    "\n",
    "Recall that we are building the skip-gram model.\n",
    "\n",
    "**We first begin by obtaining the set of contexts and focus words.**\n",
    "* Let's say we have a sentence (represented as vocabulary indicies): `[0, 2, 3, 6, 7]`.\n",
    "* For every word in the sentence, we want to get the words which are `window_size` around it.\n",
    "* So if `window_size==2`, for the word '0', we obtain: `[[0, 2], [0, 3]]`\n",
    "* For the word '2', we obtain: `[[2, 0], [2, 3], [2, 6]]`\n",
    "* For the word '3', we obtain: `[[3, 0], [3, 2], [3, 6], [3, 7]]`\n",
    "\n",
    "Q. Print some of the index pairs and trace them back to their words.\n",
    "\n",
    "\n",
    "##### 10 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1737550849252,
     "user": {
      "displayName": "Eder Tarifa Fernandez",
      "userId": "12042907008888851310"
     },
     "user_tz": 0
    },
    "id": "wrtOwTUsyArb",
    "outputId": "e26b8e7b-3cc5-41b8-d20d-72c620a4da11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1]\n",
      " [ 0  2]\n",
      " [ 1  0]\n",
      " [ 1  2]\n",
      " [ 1  3]\n",
      " [ 2  0]\n",
      " [ 2  1]\n",
      " [ 2  3]\n",
      " [ 3  1]\n",
      " [ 3  2]\n",
      " [ 4  1]\n",
      " [ 4  2]\n",
      " [ 1  4]\n",
      " [ 1  2]\n",
      " [ 1  5]\n",
      " [ 2  4]\n",
      " [ 2  1]\n",
      " [ 2  5]\n",
      " [ 5  1]\n",
      " [ 5  2]\n",
      " [ 0  1]\n",
      " [ 0  2]\n",
      " [ 1  0]\n",
      " [ 1  2]\n",
      " [ 1  6]\n",
      " [ 2  0]\n",
      " [ 2  1]\n",
      " [ 2  6]\n",
      " [ 6  1]\n",
      " [ 6  2]\n",
      " [ 4  7]\n",
      " [ 4  2]\n",
      " [ 7  4]\n",
      " [ 7  2]\n",
      " [ 7  8]\n",
      " [ 2  4]\n",
      " [ 2  7]\n",
      " [ 2  8]\n",
      " [ 8  7]\n",
      " [ 8  2]\n",
      " [ 9  1]\n",
      " [ 9 10]\n",
      " [ 1  9]\n",
      " [ 1 10]\n",
      " [ 1 11]\n",
      " [10  9]\n",
      " [10  1]\n",
      " [10 11]\n",
      " [10 12]\n",
      " [11  1]\n",
      " [11 10]\n",
      " [11 12]\n",
      " [11 13]\n",
      " [12 10]\n",
      " [12 11]\n",
      " [12 13]\n",
      " [13 11]\n",
      " [13 12]\n",
      " [14  1]\n",
      " [14 10]\n",
      " [ 1 14]\n",
      " [ 1 10]\n",
      " [ 1 11]\n",
      " [10 14]\n",
      " [10  1]\n",
      " [10 11]\n",
      " [10 12]\n",
      " [11  1]\n",
      " [11 10]\n",
      " [11 12]\n",
      " [11 15]\n",
      " [12 10]\n",
      " [12 11]\n",
      " [12 15]\n",
      " [15 11]\n",
      " [15 12]\n",
      " [16  1]\n",
      " [16 10]\n",
      " [ 1 16]\n",
      " [ 1 10]\n",
      " [ 1 11]\n",
      " [10 16]\n",
      " [10  1]\n",
      " [10 11]\n",
      " [10 12]\n",
      " [11  1]\n",
      " [11 10]\n",
      " [11 12]\n",
      " [11 17]\n",
      " [12 10]\n",
      " [12 11]\n",
      " [12 17]\n",
      " [17 11]\n",
      " [17 12]\n",
      " [18 19]\n",
      " [18 20]\n",
      " [19 18]\n",
      " [19 20]\n",
      " [19 21]\n",
      " [20 18]\n",
      " [20 19]\n",
      " [20 21]\n",
      " [20 22]\n",
      " [21 19]\n",
      " [21 20]\n",
      " [21 22]\n",
      " [21 23]\n",
      " [22 20]\n",
      " [22 21]\n",
      " [22 23]\n",
      " [22 24]\n",
      " [23 21]\n",
      " [23 22]\n",
      " [23 24]\n",
      " [24 22]\n",
      " [24 23]\n",
      " [ 4 25]\n",
      " [ 4 26]\n",
      " [25  4]\n",
      " [25 26]\n",
      " [25 10]\n",
      " [26  4]\n",
      " [26 25]\n",
      " [26 10]\n",
      " [26 24]\n",
      " [10 25]\n",
      " [10 26]\n",
      " [10 24]\n",
      " [24 26]\n",
      " [24 10]]\n"
     ]
    }
   ],
   "source": [
    "window_size = 2\n",
    "\n",
    "idx_pairs = []\n",
    "\n",
    "# variables of interest:\n",
    "#   center_word_pos: center word position\n",
    "#   context_word_pos: context_word_position\n",
    "#   add sentence length as a constraint\n",
    "\n",
    "for sentence in tokenized_corpus:\n",
    "    indices = [word2idx[word] for word in sentence]\n",
    "\n",
    "    for center_word_pos in range(len(indices)):\n",
    "\n",
    "        for w in range(-window_size, window_size + 1):\n",
    "            context_word_pos = center_word_pos + w\n",
    "\n",
    "            if context_word_pos < 0 or context_word_pos >= len(indices) or center_word_pos == context_word_pos:\n",
    "                continue\n",
    "\n",
    "            context_word_idx = indices[context_word_pos]\n",
    "            idx_pairs.append((indices[center_word_pos], context_word_idx))\n",
    "\n",
    "idx_pairs = np.array(idx_pairs) # it will be useful to have this as numpy array\n",
    "\n",
    "print(idx_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1737550849253,
     "user": {
      "displayName": "Eder Tarifa Fernandez",
      "userId": "12042907008888851310"
     },
     "user_tz": 0
    },
    "id": "crCsfh5Uy0pc",
    "outputId": "e5b6ae7f-a89b-45b9-d3db-1dfcbce18021"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([12, 10]), array([12, 11]), array([11, 12]), array([20, 19]), array([ 4, 26])]\n",
      "[['of', 'the'], ['of', 'capital'], ['capital', 'of'], ['eat', 'will'], ['she', 'like']]\n"
     ]
    }
   ],
   "source": [
    "# Answer Q here.\n",
    "# Sample 5 elements at random and trace them back to their word pairs.\n",
    "from random import sample\n",
    "\n",
    "random_pairs = sample(list(idx_pairs), 5)\n",
    "print(random_pairs)\n",
    "\n",
    "# TODO: Trace these samples back to their word pairs.\n",
    "\n",
    "word_pairs = []\n",
    "for random_pair in random_pairs:\n",
    "  word1 = idx2word[random_pair[0]]\n",
    "  word2 = idx2word[random_pair[1]]\n",
    "  word_pairs.extend([[word1, word2]])\n",
    "print(word_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rLzHkq6FRULl"
   },
   "source": [
    "# Parameters and hyperparameters\n",
    "\n",
    "* For our toy task, let us set the embedding dimensions to 5\n",
    "* Let us run the algorithm for 10 epochs (number of times the training algorithm looks at the corpus/training data)\n",
    "* Let us choose the learning rate as 0.001\n",
    "\n",
    "We have two parameter matrices $W_1$ and $W_2$ - the embedding matrix and the weight matrix.\n",
    "\n",
    "Q. What are the dimensionalities of $W_1$ and $W_2$?\n",
    "\n",
    "\n",
    "\n",
    "##### 3 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1737550849253,
     "user": {
      "displayName": "Eder Tarifa Fernandez",
      "userId": "12042907008888851310"
     },
     "user_tz": 0
    },
    "id": "1kBhE6FeRuDu"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters:\n",
    "embedding_dims = 20\n",
    "num_epochs = 1000\n",
    "learning_rate = 0.001\n",
    "\n",
    "# The two weight matrices:\n",
    "W1 = torch.randn(embedding_dims, vocabulary_size, requires_grad=True)\n",
    "W2 = torch.randn(vocabulary_size, embedding_dims, requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "npzWXeQATPs4"
   },
   "source": [
    "# Training the model\n",
    "\n",
    "(Refer to Lecture 2 slides 30-31)\n",
    "\n",
    "In the code below, we are going to compute the log probability of the correct context (target) given the word.\n",
    "\n",
    "Before running the code, answer the question commented in the code -> fill `y_true`.\n",
    "\n",
    "Print the loss and see if the loss goes down.\n",
    "\n",
    "##### 10 mins\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 70603,
     "status": "ok",
     "timestamp": 1737550919852,
     "user": {
      "displayName": "Eder Tarifa Fernandez",
      "userId": "12042907008888851310"
     },
     "user_tz": 0
    },
    "id": "5Cx2eJi1a8R6",
    "outputId": "cc65c484-bdef-4d73-fa69-acdb5795c254"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 0: 9.182967710566635\n",
      "Loss at epoch 10: 7.00546034265023\n",
      "Loss at epoch 20: 5.745183305155773\n",
      "Loss at epoch 30: 4.92009689784967\n",
      "Loss at epoch 40: 4.316935610312682\n",
      "Loss at epoch 50: 3.868524597126704\n",
      "Loss at epoch 60: 3.5323581285201584\n",
      "Loss at epoch 70: 3.2798559844493864\n",
      "Loss at epoch 80: 3.084092174126552\n",
      "Loss at epoch 90: 2.926767704807795\n",
      "Loss at epoch 100: 2.8002222780997936\n",
      "Loss at epoch 110: 2.6954578062662713\n",
      "Loss at epoch 120: 2.605726859202752\n",
      "Loss at epoch 130: 2.5270517855882644\n",
      "Loss at epoch 140: 2.456618318878687\n",
      "Loss at epoch 150: 2.3924814393887153\n",
      "Loss at epoch 160: 2.3333781843002024\n",
      "Loss at epoch 170: 2.2784937069966245\n",
      "Loss at epoch 180: 2.2272899439701668\n",
      "Loss at epoch 190: 2.1793969039733594\n",
      "Loss at epoch 200: 2.134549072614083\n",
      "Loss at epoch 210: 2.0925440618625055\n",
      "Loss at epoch 220: 2.053217203112749\n",
      "Loss at epoch 230: 2.0164282874419137\n",
      "Loss at epoch 240: 1.9820541395590856\n",
      "Loss at epoch 250: 1.9499837877658697\n",
      "Loss at epoch 260: 1.9201165543152736\n",
      "Loss at epoch 270: 1.8923610198956269\n",
      "Loss at epoch 280: 1.8666226217379938\n",
      "Loss at epoch 290: 1.8427974054446588\n",
      "Loss at epoch 300: 1.8207663098206888\n",
      "Loss at epoch 310: 1.8004036417374245\n",
      "Loss at epoch 320: 1.781593217758032\n",
      "Loss at epoch 330: 1.7642383529589727\n",
      "Loss at epoch 340: 1.748262619284483\n",
      "Loss at epoch 350: 1.7336042924569204\n",
      "Loss at epoch 360: 1.7202049360825464\n",
      "Loss at epoch 370: 1.7079986297167264\n",
      "Loss at epoch 380: 1.6969073744920584\n",
      "Loss at epoch 390: 1.6868401976732108\n",
      "Loss at epoch 400: 1.6777002738072322\n",
      "Loss at epoch 410: 1.6693884955002711\n",
      "Loss at epoch 420: 1.66180963745484\n",
      "Loss at epoch 430: 1.654876714486342\n",
      "Loss at epoch 440: 1.648512567465122\n",
      "Loss at epoch 450: 1.6426502081064078\n",
      "Loss at epoch 460: 1.6372343870309682\n",
      "Loss at epoch 470: 1.6322179679687208\n",
      "Loss at epoch 480: 1.627563471060533\n",
      "Loss at epoch 490: 1.623238567664073\n",
      "Loss at epoch 500: 1.6192163962584276\n",
      "Loss at epoch 510: 1.6154733061790467\n",
      "Loss at epoch 520: 1.6119874656200408\n",
      "Loss at epoch 530: 1.608739161491394\n",
      "Loss at epoch 540: 1.6057100548194005\n",
      "Loss at epoch 550: 1.6028824943762559\n",
      "Loss at epoch 560: 1.6002400884261498\n",
      "Loss at epoch 570: 1.5977674768521235\n",
      "Loss at epoch 580: 1.5954500688956335\n",
      "Loss at epoch 590: 1.593274696973654\n",
      "Loss at epoch 600: 1.5912293136119842\n",
      "Loss at epoch 610: 1.5893025971375978\n",
      "Loss at epoch 620: 1.587484610080719\n",
      "Loss at epoch 630: 1.585766435586489\n",
      "Loss at epoch 640: 1.5841398129096398\n",
      "Loss at epoch 650: 1.582597063596432\n",
      "Loss at epoch 660: 1.5811320103131807\n",
      "Loss at epoch 670: 1.5797388553619385\n",
      "Loss at epoch 680: 1.578412188933446\n",
      "Loss at epoch 690: 1.5771472995097822\n",
      "Loss at epoch 700: 1.575939797438108\n",
      "Loss at epoch 710: 1.574785965681076\n",
      "Loss at epoch 720: 1.573682045478087\n",
      "Loss at epoch 730: 1.5726251661777497\n",
      "Loss at epoch 740: 1.5716122893186717\n",
      "Loss at epoch 750: 1.5706406698777124\n",
      "Loss at epoch 760: 1.569707746230639\n",
      "Loss at epoch 770: 1.568811494570512\n",
      "Loss at epoch 780: 1.567949801683426\n",
      "Loss at epoch 790: 1.5671205965372232\n",
      "Loss at epoch 800: 1.5663222290002383\n",
      "Loss at epoch 810: 1.5655530512332916\n",
      "Loss at epoch 820: 1.564811398432805\n",
      "Loss at epoch 830: 1.564096095928779\n",
      "Loss at epoch 840: 1.5634057961977446\n",
      "Loss at epoch 850: 1.5627390247124893\n",
      "Loss at epoch 860: 1.5620949933162103\n",
      "Loss at epoch 870: 1.561472312762187\n",
      "Loss at epoch 880: 1.5608700697238629\n",
      "Loss at epoch 890: 1.5602872848510743\n",
      "Loss at epoch 900: 1.5597230952519636\n",
      "Loss at epoch 910: 1.559176754951477\n",
      "Loss at epoch 920: 1.558647382259369\n",
      "Loss at epoch 930: 1.5581342046077435\n",
      "Loss at epoch 940: 1.5576365292072296\n",
      "Loss at epoch 950: 1.5571536059562976\n",
      "Loss at epoch 960: 1.5566849603102757\n",
      "Loss at epoch 970: 1.5562301438588362\n",
      "Loss at epoch 980: 1.5557883019630725\n",
      "Loss at epoch 990: 1.5553588825922746\n",
      "\n",
      "Final epoch loss: 1.5549828047935779\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "\n",
    "    loss_val = 0\n",
    "\n",
    "    for data, target in idx_pairs:\n",
    "\n",
    "        x = torch.Tensor(look_up_table(data)) # x is a One-hot tensor\n",
    "\n",
    "        y_true = torch.Tensor([target]).long()\n",
    "        # Q. what would y_true be? The actual word\n",
    "\n",
    "        z1 = torch.matmul(W1, x)\n",
    "        # Q. what is z1? The result of the word being processed by the first layer, staying just with the encoded word -> embedded word\n",
    "\n",
    "        z2 = torch.matmul(W2, z1)\n",
    "        # Q. what is the above operation? Context matrix\n",
    "\n",
    "        # Let us obtain prediction over the vocabulary\n",
    "        log_softmax = F.log_softmax(z2, dim=0)\n",
    "\n",
    "        # Our loss is a negative log-likelihood loss\n",
    "        # (what does this mean?) We want to minimize the error using a probability of multiple classes, we use the log to avoid exploding gradient\n",
    "\n",
    "        loss = F.nll_loss(log_softmax.view(1,-1), y_true)\n",
    "\n",
    "        loss_val += loss.item()\n",
    "\n",
    "        # propagate the error\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient descent\n",
    "        W1.data -= learning_rate * W1.grad.data\n",
    "        W2.data -= learning_rate * W2.grad.data\n",
    "\n",
    "        # zero out gradient accumulation\n",
    "        W1.grad.data.zero_()\n",
    "        W2.grad.data.zero_()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Loss at epoch {epoch}: {loss_val/len(idx_pairs)}')\n",
    "\n",
    "print(f'\\nFinal epoch loss: {loss_val/len(idx_pairs)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JPsXq60HUWWq"
   },
   "source": [
    "Q. Given that we are interested in distributed representations, what is the major bottleneck in our setup? Is it the dimensionality of the representations? Is it the learning rate? Is it the corpus?\n",
    "\n",
    "Q. What hyperparameters would you tune to improve the representations?  The size of the embedding is the problem, it is too small\n",
    "\n",
    "Q. Train the algorithm with a bigger corpus.\n",
    "\n",
    "##### 10 mins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ZcpFDLcy0pd"
   },
   "source": [
    "You can either copy and paste the corpus and bring it to the same format as the corpus above or use following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1737550919852,
     "user": {
      "displayName": "Eder Tarifa Fernandez",
      "userId": "12042907008888851310"
     },
     "user_tz": 0
    },
    "id": "ixhvaYkW_SfX"
   },
   "outputs": [],
   "source": [
    "# Example code for getting corpora from the internet\n",
    "# import urllib\n",
    "# txt = [line.strip() for line in urllib.request.urlopen('https://raw.githubusercontent.com/luonglearnstocode/Seinfeld-text-corpus/master/corpus.txt').readlines()]\n",
    "# decoded_txt = [line.decode('utf-8') for line in txt]\n",
    "# lines = 0\n",
    "# for line in decoded_txt:\n",
    "#   lines += 1\n",
    "# print(lines)\n",
    "# print(decoded_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1737550919852,
     "user": {
      "displayName": "Eder Tarifa Fernandez",
      "userId": "12042907008888851310"
     },
     "user_tz": 0
    },
    "id": "YNeDoTVjrO-_"
   },
   "outputs": [],
   "source": [
    "# re_punctuation_string = '[\\s,/.\\']'\n",
    "\n",
    "# # PR = punctuation removed\n",
    "# tokenized_corpus_txt = []\n",
    "# for sentence in decoded_txt:\n",
    "#     tokenized_sentence_PR = re.split(re_punctuation_string, sentence) # in python's regex, [...] is an alternative to writing .|.|.\n",
    "#     tokenized_sentence_PR = list(filter(None, tokenized_sentence_PR)) # remove empty strings from list\n",
    "#     tokenized_corpus_txt.append(tokenized_sentence_PR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1737550919852,
     "user": {
      "displayName": "Eder Tarifa Fernandez",
      "userId": "12042907008888851310"
     },
     "user_tz": 0
    },
    "id": "PL8uD01Gt5K8"
   },
   "outputs": [],
   "source": [
    "# tokenized_corpus_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1737550919852,
     "user": {
      "displayName": "Eder Tarifa Fernandez",
      "userId": "12042907008888851310"
     },
     "user_tz": 0
    },
    "id": "bcTs-8gZu5N4"
   },
   "outputs": [],
   "source": [
    "# vocabulary = set() # Let us put all the tokens (mostly words)\n",
    "#                 # appearing in the vocabulary in a list\n",
    "\n",
    "# for sentence in tokenized_corpus_txt:\n",
    "#     for token in sentence:\n",
    "#       vocabulary.add(token)\n",
    "# print(vocabulary)\n",
    "# print(len(vocabulary))  # Abismal difference of execution time with sets\n",
    "# vocabulary_size = len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1737550919852,
     "user": {
      "displayName": "Eder Tarifa Fernandez",
      "userId": "12042907008888851310"
     },
     "user_tz": 0
    },
    "id": "Fh446Zdqx6WP"
   },
   "outputs": [],
   "source": [
    "# def look_up_table(word_idx):\n",
    "#     x = torch.zeros(vocabulary_size).float()\n",
    "#     x[word_idx] = 1.0\n",
    "#     return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1737550919852,
     "user": {
      "displayName": "Eder Tarifa Fernandez",
      "userId": "12042907008888851310"
     },
     "user_tz": 0
    },
    "id": "aLWyXwM5quRy"
   },
   "outputs": [],
   "source": [
    "\n",
    "# word2idx = {w: idx for (idx, w) in enumerate(vocabulary)}\n",
    "# idx2word = {idx: w for (idx, w) in enumerate(vocabulary)}\n",
    "\n",
    "# print(word2idx)\n",
    "# print(idx2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1737550919852,
     "user": {
      "displayName": "Eder Tarifa Fernandez",
      "userId": "12042907008888851310"
     },
     "user_tz": 0
    },
    "id": "iUH6q9Crp70q"
   },
   "outputs": [],
   "source": [
    "# window_size = 2\n",
    "\n",
    "# idx_pairs = []\n",
    "\n",
    "# # variables of interest:\n",
    "# #   center_word_pos: center word position\n",
    "# #   context_word_pos: context_word_position\n",
    "# #   add sentence length as a constraint\n",
    "\n",
    "# for sentence in tokenized_corpus_txt:\n",
    "#     indices = [word2idx[word] for word in sentence]\n",
    "\n",
    "#     for center_word_pos in range(len(indices)):\n",
    "\n",
    "#         for w in range(-window_size, window_size + 1):\n",
    "#             context_word_pos = center_word_pos + w\n",
    "\n",
    "#             if context_word_pos < 0 or context_word_pos >= len(indices) or center_word_pos == context_word_pos:\n",
    "#                 continue\n",
    "\n",
    "#             context_word_idx = indices[context_word_pos]\n",
    "#             idx_pairs.append((indices[center_word_pos], context_word_idx))\n",
    "\n",
    "# idx_pairs = np.array(idx_pairs) # it will be useful to have this as numpy array\n",
    "\n",
    "# print(idx_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1737550919852,
     "user": {
      "displayName": "Eder Tarifa Fernandez",
      "userId": "12042907008888851310"
     },
     "user_tz": 0
    },
    "id": "1u1l3VPNvvlM"
   },
   "outputs": [],
   "source": [
    "# # Hyperparameters:\n",
    "# embedding_dims = 15\n",
    "# num_epochs = 5\n",
    "# learning_rate = 0.01\n",
    "\n",
    "# # The two weight matrices:\n",
    "# W1 = torch.randn(embedding_dims, vocabulary_size, requires_grad=True)\n",
    "# W2 = torch.randn(vocabulary_size, embedding_dims, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u2znwTJeU3UT"
   },
   "source": [
    "# Using word embeddings\n",
    "\n",
    "One of the simplest ways of exploiting word representations is to find similar words. There are many ways of measuring the semantic similarity between two words. As we are using word representations which are vectors in the euclidean space, distance metrics defined in the euclidean space are the most popular choice. This is because words that share common contexts in the corpus are located in close proximity to one another in the euclidean space.  One such metric is the eucldeian distance.\n",
    "\n",
    "Q. What is the euclidean distance between 'the' and 'a' (in the sample corpus and the new corpus)? Reflect on their differences.  ??\n",
    "\n",
    "Q. What other distance metrics can we use for two vectors? Cosine\n",
    "\n",
    "\n",
    "\n",
    "##### 10 mins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1737550919852,
     "user": {
      "displayName": "Eder Tarifa Fernandez",
      "userId": "12042907008888851310"
     },
     "user_tz": 0
    },
    "id": "NUPETZaOgvgB",
    "outputId": "842b9b1b-4e31-493d-84fe-4ffd86f61aa8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.278243541717529\n"
     ]
    }
   ],
   "source": [
    "# Let us get two vectors from the trained model\n",
    "\n",
    "x = torch.Tensor(look_up_table(0))\n",
    "x_emb = torch.matmul(W1, x).detach().numpy()\n",
    "y = torch.Tensor(look_up_table(1))\n",
    "y_emb = torch.matmul(W1, y).detach().numpy()\n",
    "\n",
    "# let us print the euclidean distance\n",
    "print(euclidean(x_emb, y_emb)) # Res: 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1737550919852,
     "user": {
      "displayName": "Eder Tarifa Fernandez",
      "userId": "12042907008888851310"
     },
     "user_tz": 0
    },
    "id": "SgSTIDlFy0pd"
   },
   "outputs": [],
   "source": [
    "# Q. What is the euclidean distance between 'the' and 'a' (sample corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1737550919852,
     "user": {
      "displayName": "Eder Tarifa Fernandez",
      "userId": "12042907008888851310"
     },
     "user_tz": 0
    },
    "id": "s6tPPvmMy0pe"
   },
   "outputs": [],
   "source": [
    "# Q. What is the euclidean distance between 'the' and 'a' (new corpus)\n",
    "# x = torch.Tensor(look_up_table(0))\n",
    "# x_emb = torch.matmul(W1, x).detach().numpy()\n",
    "# y = torch.Tensor(look_up_table(1))\n",
    "# y_emb = torch.matmul(W1, y).detach().numpy()\n",
    "\n",
    "# # let us print the euclidean distance\n",
    "# print(euclidean(x_emb, y_emb)) # Res: 7.17 --> hiher dim?? check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7UZroMvLtFVv"
   },
   "source": [
    "# ADVANCED: Training with negative sampling\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Refer to skipgram models in the slides.\n",
    "\n",
    "Q. What happens when we have a very large vocabulary? Computing the softmax funtion is extremely expensive\n",
    "\n",
    "Q. What is a negative sample? A way of replacing the softmax function for a sigmoid function for each word that should appear in the context of the target embedding and for the noisy words (k = n of noisy words). This reduces computational cost a lot.\n",
    "\n",
    "\n",
    "Below is the code for training the model with negative sampling.\n",
    "\n",
    "\n",
    "##### 10 mins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1737550919852,
     "user": {
      "displayName": "Eder Tarifa Fernandez",
      "userId": "12042907008888851310"
     },
     "user_tz": 0
    },
    "id": "uthqU7FP8s54",
    "outputId": "0bd0bdec-58e5-4384-fda6-6afe1a48a751"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 923
    },
    "executionInfo": {
     "elapsed": 36050,
     "status": "error",
     "timestamp": 1737550955898,
     "user": {
      "displayName": "Eder Tarifa Fernandez",
      "userId": "12042907008888851310"
     },
     "user_tz": 0
    },
    "id": "Lq1lcWbqRwNY",
    "outputId": "cff9c993-c71c-4aca-c8a6-b106ed0e66e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 0: 3.4653482764565315\n",
      "Loss at epoch 10: 3.1629490110611256\n",
      "Loss at epoch 20: 2.7135122798383238\n",
      "Loss at epoch 30: 2.8822830957002363\n",
      "Loss at epoch 40: 2.5501048560325916\n",
      "Loss at epoch 50: 2.4085746922136213\n",
      "Loss at epoch 60: 2.2398041631363763\n",
      "Loss at epoch 70: 2.22645751937794\n",
      "Loss at epoch 80: 1.844106201043066\n",
      "Loss at epoch 90: 1.9829661493404553\n",
      "Loss at epoch 100: 1.9941923734937939\n",
      "Loss at epoch 110: 1.7761964209377765\n",
      "Loss at epoch 120: 1.7467388963523822\n",
      "Loss at epoch 130: 1.670601920210398\n",
      "Loss at epoch 140: 1.6782184679491015\n",
      "Loss at epoch 150: 1.7948420465350725\n",
      "Loss at epoch 160: 1.3815086213585275\n",
      "Loss at epoch 170: 1.3519017037299748\n",
      "Loss at epoch 180: 1.3884420606092764\n",
      "Loss at epoch 190: 1.3425367538745587\n",
      "Loss at epoch 200: 1.467068009594312\n",
      "Loss at epoch 210: 1.2983398839974634\n",
      "Loss at epoch 220: 1.3470229580711859\n",
      "Loss at epoch 230: 1.165724522641036\n",
      "Loss at epoch 240: 1.3344883180008484\n",
      "Loss at epoch 250: 1.3262900595195017\n",
      "Loss at epoch 260: 1.2611896827888602\n",
      "Loss at epoch 270: 1.3767055432956952\n",
      "Loss at epoch 280: 1.2862062063211432\n",
      "Loss at epoch 290: 1.3138948783278466\n",
      "Loss at epoch 300: 1.0949428499031526\n",
      "Loss at epoch 310: 1.3353894191077695\n",
      "Loss at epoch 320: 1.1007464241558829\n",
      "Loss at epoch 330: 1.0074378703649227\n",
      "Loss at epoch 340: 1.2266973185711183\n",
      "Loss at epoch 350: 1.255486275556569\n",
      "Loss at epoch 360: 1.1333989541117961\n",
      "Loss at epoch 370: 1.2018695677702242\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-6ed8f68fbca6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mneg_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0my_neg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mneg_sample\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0my_neg_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlook_up_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneg_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# The two weight matrices:\n",
    "W1 = torch.randn(embedding_dims, vocabulary_size, requires_grad=True)\n",
    "W2 = torch.randn(embedding_dims, vocabulary_size, requires_grad=True)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    for data, target in idx_pairs:\n",
    "        x_var = Variable(look_up_table(data)).float()\n",
    "\n",
    "        y_pos = Variable(torch.from_numpy(np.array([target])).long())\n",
    "        y_pos_var = Variable(look_up_table(target)).float()\n",
    "\n",
    "        neg_sample = np.random.choice(list(range(vocabulary_size)),size=(1))[0]\n",
    "        y_neg = Variable(torch.from_numpy(np.array([neg_sample])))\n",
    "        y_neg_var = Variable(look_up_table(neg_sample)).float()\n",
    "\n",
    "        x_emb = torch.matmul(W1, x_var)\n",
    "        y_pos_emb = torch.matmul(W2, y_pos_var)\n",
    "        y_neg_emb = torch.matmul(W2, y_neg_var)\n",
    "\n",
    "        # get positive sample score\n",
    "        pos_loss = F.logsigmoid(torch.matmul(x_emb, y_pos_emb))\n",
    "\n",
    "        # get negsample score\n",
    "        neg_loss = F.logsigmoid(-1 * torch.matmul(x_emb, y_neg_emb))\n",
    "\n",
    "        loss = - (pos_loss + neg_loss)\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # propagate the error\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient descent\n",
    "        W1.data -= learning_rate * W1.grad.data\n",
    "        W2.data -= learning_rate * W2.grad.data\n",
    "\n",
    "        # zero out gradient accumulation\n",
    "        W1.grad.data.zero_()\n",
    "        W2.grad.data.zero_()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Loss at epoch {epoch}: {epoch_loss/len(idx_pairs)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kGgcB8VHXvPK"
   },
   "source": [
    "* In the current setup, we are only exploiting a very small sample of negative examples. This is suboptimal.\n",
    "\n",
    "* Given a sufficiently large vocabulary, we would ideally sample the negative samples from a noise distribution whose probabilities match the frequency of vocabulary.\n",
    "\n",
    "\n",
    "Q.  Using this code as the basis, build an object oriented negative sampling based model and train it on the fairly large corpus.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 288566,
     "status": "ok",
     "timestamp": 1737551249176,
     "user": {
      "displayName": "Eder Tarifa Fernandez",
      "userId": "12042907008888851310"
     },
     "user_tz": 0
    },
    "id": "Y_Eknuozunq5",
    "outputId": "7e1f2e83-a3fa-4791-d26c-d1b0f153de1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-01-22 13:02:40--  http://nlp.stanford.edu/data/glove.6B.zip\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
      "--2025-01-22 13:02:40--  https://nlp.stanford.edu/data/glove.6B.zip\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
      "--2025-01-22 13:02:40--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 862182613 (822M) [application/zip]\n",
      "Saving to: glove.6B.zip.1\n",
      "\n",
      "glove.6B.zip.1      100%[===================>] 822.24M  5.01MB/s    in 2m 39s  \n",
      "\n",
      "2025-01-22 13:05:19 (5.18 MB/s) - glove.6B.zip.1 saved [862182613/862182613]\n",
      "\n",
      "Archive:  glove.6B.zip\n",
      "replace glove.6B.50d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
      "  inflating: glove.6B.50d.txt        \n",
      "replace glove.6B.100d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
      "  inflating: glove.6B.100d.txt       y\n",
      "\n",
      "replace glove.6B.200d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename:   inflating: glove.6B.200d.txt       y\n",
      "\n",
      "replace glove.6B.300d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename:   inflating: glove.6B.300d.txt       y\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1737551250021,
     "user": {
      "displayName": "Eder Tarifa Fernandez",
      "userId": "12042907008888851310"
     },
     "user_tz": 0
    },
    "id": "31U037qV9Nws"
   },
   "outputs": [],
   "source": [
    "def tokenize(corpus):\n",
    "  re_punctuation_string = '[\\s,/.\\']'\n",
    "  tokenized_corpus_txt = []\n",
    "  #decoded_txt = [line.decode('utf-8') for line in corpus]\n",
    "  for line in tqdm(corpus.readlines()):\n",
    "    # Ignore the first line - first line typically contains vocab, dimensionality\n",
    "    if len(line.strip().split()) > 3:\n",
    "        tokenized_sentence_PR = re.split(re_punctuation_string, line) # in python's regex, [...] is an alternative to writing .|.|.\n",
    "        tokenized_sentence_PR = list(filter(None, tokenized_sentence_PR)) # remove empty strings from list\n",
    "        tokenized_corpus_txt.append(tokenized_sentence_PR)\n",
    "  return tokenized_corpus_txt\n",
    "\n",
    "def set_vocabulary(tokenized_corpus):\n",
    "  vocabulary = set() # Let us put all the tokens (mostly words)\n",
    "                  # appearing in the vocabulary in a list\n",
    "\n",
    "  for sentence in tokenized_corpus:\n",
    "      for token in sentence:\n",
    "        vocabulary.add(token)\n",
    "  vocabulary_size = len(vocabulary)\n",
    "\n",
    "def look_up_table(word_idx):\n",
    "    x = torch.zeros(vocabulary_size).float()\n",
    "    x[word_idx] = 1.0\n",
    "    return x\n",
    "\n",
    "def set_idx_pairs(tokenized_corpus):\n",
    "\n",
    "  window_size = 2\n",
    "\n",
    "  idx_pairs = []\n",
    "\n",
    "  for sentence in tokenized_corpus:\n",
    "      indices = [word2idx[word] for word in sentence]\n",
    "\n",
    "      for center_word_pos in range(len(indices)):\n",
    "\n",
    "          for w in range(-window_size, window_size + 1):\n",
    "              context_word_pos = center_word_pos + w\n",
    "\n",
    "              if context_word_pos < 0 or context_word_pos >= len(indices) or center_word_pos == context_word_pos:\n",
    "                  continue\n",
    "\n",
    "              context_word_idx = indices[context_word_pos]\n",
    "              idx_pairs.append((indices[center_word_pos], context_word_idx))\n",
    "\n",
    "  idx_pairs = np.array(idx_pairs) # it will be useful to have this as numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1737551250021,
     "user": {
      "displayName": "Eder Tarifa Fernandez",
      "userId": "12042907008888851310"
     },
     "user_tz": 0
    },
    "id": "WnKS1EQn_m7P"
   },
   "outputs": [],
   "source": [
    "# # this is a large file, it will take a while to load in the memory!\n",
    "# with codecs.open('glove.6B.50d.txt', 'r','utf-8') as f:\n",
    "#   index = 0\n",
    "#   tokenized_corpus = tokenize(f)\n",
    "#   print(tokenized_corpus)\n",
    "#   # set_vocabulary(tokenized_corpus=tokenized_corpus)\n",
    "#   # print(vocabulary_size)\n",
    "#   # word2idx = {w: idx for (idx, w) in enumerate(vocabulary)}\n",
    "#   # idx2word = {idx: w for (idx, w) in enumerate(vocabulary)}\n",
    "#   # set_idx_pairs(tokenized_corpus=tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1737551250021,
     "user": {
      "displayName": "Eder Tarifa Fernandez",
      "userId": "12042907008888851310"
     },
     "user_tz": 0
    },
    "id": "6_9VmbObA2Jt",
    "outputId": "72869026-6127-44f5-8125-cc558705d1d0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 701
    },
    "executionInfo": {
     "elapsed": 223057,
     "status": "error",
     "timestamp": 1737552085203,
     "user": {
      "displayName": "Eder Tarifa Fernandez",
      "userId": "12042907008888851310"
     },
     "user_tz": 0
    },
    "id": "qhwlZL6p_ZYe",
    "outputId": "86d1de6c-dc67-4532-da04-06177f74a8a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 0: 34.637506396953874\n",
      "Loss at epoch 10: 20.489668721419115\n",
      "Loss at epoch 20: 13.417021434123699\n",
      "Loss at epoch 30: 9.81436321735382\n",
      "Loss at epoch 40: 8.35036869599269\n",
      "Loss at epoch 50: 6.2288029854114235\n",
      "Loss at epoch 60: 5.310239357214708\n",
      "Loss at epoch 70: 4.652064749827752\n",
      "Loss at epoch 80: 4.098232934108148\n",
      "Loss at epoch 90: 3.926960084071526\n",
      "Loss at epoch 100: 3.701324500487401\n",
      "Loss at epoch 110: 3.4742994849498454\n",
      "Loss at epoch 120: 3.3432182486240682\n",
      "Loss at epoch 130: 3.269650130088513\n",
      "Loss at epoch 140: 3.211483122752263\n",
      "Loss at epoch 150: 3.059455436926622\n",
      "Loss at epoch 160: 3.023767922474788\n",
      "Loss at epoch 170: 3.0083436058117794\n",
      "Loss at epoch 180: 2.936440192736112\n",
      "Loss at epoch 190: 2.911389868076031\n",
      "Loss at epoch 200: 2.8877983065751884\n",
      "Loss at epoch 210: 2.8458718098126923\n",
      "Loss at epoch 220: 2.772542239152468\n",
      "Loss at epoch 230: 2.7805497077795174\n",
      "Loss at epoch 240: 2.766512055580433\n",
      "Loss at epoch 250: 2.7000071883201597\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-9cae3181ad8d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m           \u001b[0my_neg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mneg_sample\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m           \u001b[0my_neg_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlook_up_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneg_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m           \u001b[0my_neg_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_neg_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m           \u001b[0mneg_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogsigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW1\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mx_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_neg_emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m### ojo que hay que cambiar esta linea para que funciones por un problema del grafo del gradiente\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# The two weight matrices:\n",
    "W1 = torch.randn(embedding_dims, vocabulary_size, requires_grad=True)\n",
    "W2 = torch.randn(embedding_dims, vocabulary_size, requires_grad=True)\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    for data, target in idx_pairs:\n",
    "        x_var = Variable(look_up_table(data)).float()\n",
    "\n",
    "        y_pos = Variable(torch.from_numpy(np.array([target])).long())\n",
    "        y_pos_var = Variable(look_up_table(target)).float()\n",
    "\n",
    "        neg_loss = 0\n",
    "        for i in range(20):\n",
    "          neg_sample = np.random.choice(list(range(vocabulary_size)),size=(1))[0]\n",
    "          y_neg = Variable(torch.from_numpy(np.array([neg_sample])))\n",
    "          y_neg_var = Variable(look_up_table(neg_sample)).float()\n",
    "          y_neg_emb = torch.matmul(W2, y_neg_var)\n",
    "          neg_loss += F.logsigmoid(-torch.matmul(W1 @ x_var, y_neg_emb)) ### ojo que hay que cambiar esta linea para que funciones por un problema del grafo del gradiente\n",
    "\n",
    "\n",
    "        y_neg = Variable(torch.from_numpy(np.array([neg_sample])))\n",
    "        y_neg_var = Variable(look_up_table(neg_sample)).float()\n",
    "\n",
    "        x_emb = torch.matmul(W1, x_var)\n",
    "        y_pos_emb = torch.matmul(W2, y_pos_var)\n",
    "\n",
    "        # get positive sample score\n",
    "        pos_loss = F.logsigmoid(torch.matmul(x_emb, y_pos_emb))\n",
    "\n",
    "        loss = - (pos_loss + neg_loss)\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # propagate the error\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient descent\n",
    "        W1.data -= learning_rate * W1.grad.data\n",
    "        W2.data -= learning_rate * W2.grad.data\n",
    "\n",
    "        # zero out gradient accumulation\n",
    "        W1.grad.data.zero_()\n",
    "        W2.grad.data.zero_()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Loss at epoch {epoch}: {epoch_loss/len(idx_pairs)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f4VCX4XSqvlu"
   },
   "source": [
    "# Pre-trained representations\n",
    "\n",
    "We have seen from the above that word embeddings are learned in an unsupervised manner, i.e., we don't have any labelled data. These representations can be used to `bootstrap' models in NLP. There are many word representation inducing algorithms : [word2vec](https://arxiv.org/abs/1301.3781), [GloVe](https://nlp.stanford.edu/pubs/glove.pdf), [Fasttext](https://arxiv.org/abs/1607.04606) are some of the popular choices. There are differences in the algorithms but they are all based on the distributional hypothesis.\n",
    "\n",
    "We will now use one of these pre-trained representations: GloVe.\n",
    "\n",
    "Q. What is the dimensionality of the representations below?\n",
    "\n",
    "##### 2 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20157,
     "status": "ok",
     "timestamp": 1737552108522,
     "user": {
      "displayName": "Eder Tarifa Fernandez",
      "userId": "12042907008888851310"
     },
     "user_tz": 0
    },
    "id": "OCsV8mtBg4-Y",
    "outputId": "4b1cde70-5ab5-4268-ec67-08a423684336"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 400000/400000 [00:15<00:00, 25038.58it/s]\n"
     ]
    }
   ],
   "source": [
    "w2i = [] # word2index\n",
    "i2w = [] # index2word\n",
    "wvecs = [] # word vectors\n",
    "\n",
    "# this is a large file, it will take a while to load in the memory!\n",
    "with codecs.open('glove.6B.50d.txt', 'r','utf-8') as f:\n",
    "  index = 0\n",
    "  for line in tqdm(f.readlines()):\n",
    "    # Ignore the first line - first line typically contains vocab, dimensionality\n",
    "    if len(line.strip().split()) > 3:\n",
    "\n",
    "      (word, vec) = (line.strip().split()[0],\n",
    "                     list(map(float,line.strip().split()[1:])))\n",
    "\n",
    "      wvecs.append(vec)\n",
    "      w2i.append((word, index))\n",
    "      i2w.append((index, word))\n",
    "      index += 1\n",
    "\n",
    "w2i = dict(w2i)\n",
    "i2w = dict(i2w)\n",
    "wvecs = np.array(wvecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2cP-UmR5Br4_"
   },
   "source": [
    "For the following experiments, we recommend  using `wvecs` - the pretrained representations.\n",
    "\n",
    "# Evaluating word representation models\n",
    "\n",
    "## Inrtinsic Evaluation\n",
    "\n",
    "* Intrinsic evaluation of word representations involves evaluating  set of word vectors generated by an embedding technique on specific  subtasks that in someways are directly related to the distributional hypothesis. These are typically simple and fast to compute and thereby allow us to help understand representation learning algorithms.\n",
    "\n",
    "* An intrinsic evaluation should typically return to us a scalar quantity that measures the performance of those word vectors on the evaluation subtask.\n",
    "\n",
    "\n",
    "\n",
    "## Word Similarity\n",
    "\n",
    "The first task we consider is evaluating if the representations are good at computing if two words are similar. In this task, you will use both euclidean distance or cosine distance as similarity measures.\n",
    "\n",
    "* Print similarity scores for word pairs in https://github.com/iraleviant/eval-multilingual-simlex/blob/master/evaluation/ws-353/wordsim353-english-sim.txt\n",
    "\n",
    "     (Format of the file: two words and the corresponding human score for the two words)\n",
    "\n",
    "* Obtain pearson's correlation with predicted scores and the human generated scores.\n",
    "\n",
    "\n",
    "##### 15 mins\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "executionInfo": {
     "elapsed": 218,
     "status": "ok",
     "timestamp": 1737552935824,
     "user": {
      "displayName": "Eder Tarifa Fernandez",
      "userId": "12042907008888851310"
     },
     "user_tz": 0
    },
    "id": "k5EGDVRuOXCD"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 198,
     "status": "ok",
     "timestamp": 1737552975453,
     "user": {
      "displayName": "Eder Tarifa Fernandez",
      "userId": "12042907008888851310"
     },
     "user_tz": 0
    },
    "id": "iR2tSAFBL4Lp",
    "outputId": "1b6e6793-76fb-4448-d450-ed8494eeff17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 1\tWord2\tCosine\tEuclidean\tPearson\n",
      "tiger \t cat \t 0.38492675815948374 \t 4.122908683917116 \t 0.6271592806407994\n",
      "plane \t plane \t 0.0 \t 0.0 \t 0.9999999999999999\n",
      "water \t pencil \t 0.7877518228411655 \t 7.017885353644521 \t 0.2680004470852179\n",
      "lion \t dinosaur \t 0.6055773368557676 \t 5.33600626592766 \t 0.39051543656939486\n"
     ]
    }
   ],
   "source": [
    "words = [[\"tiger\", \"cat\"], [\"plane\", \"plane\"], [\"water\", \"pencil\"], [\"lion\", \"dinosaur\"]]\n",
    "print(\"Word 1\\tWord2\\tCosine\\tEuclidean\\tPearson\")\n",
    "for word in words:\n",
    "  emb1 = wvecs[w2i[word[0]]]\n",
    "  emb2 = wvecs[w2i[word[1]]]\n",
    "  print(word[0], \"\\t\", word [1], \"\\t\", cosine(emb1, emb2), \"\\t\", euclidean(emb1, emb2), \"\\t\", pearsonr(emb1, emb2)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S8Kev8-wA1QA"
   },
   "source": [
    "##  Exploring Analogies\n",
    "\n",
    "The second task we consider **completing analogies**. We are given an incomplete analogy of the form:\n",
    "\n",
    "\n",
    "* $a : b : : c :~?$\n",
    "\n",
    "\n",
    "We would then identify the word vector which maximizes the cosine similarity.\n",
    "This metric has an intuitive interpretation. Ideally, we want $\\phi(b) - \\phi(a) = \\phi(d) - \\phi(c)$ where $\\phi(.)$ is the word vector.\n",
    "For instance,\n",
    "\n",
    "* *london $-$ england = paris $-$ france* .\n",
    "\n",
    "Thus we identify the vector $\\phi(d)$ which maximizes the normalized dot-product between the two word\n",
    "vectors (i.e. cosine similarity).\n",
    "\n",
    "\n",
    "\n",
    "* You can either use your own method to compute the correct word or use the code below.\n",
    "\n",
    "* Use original analogies dataset https://github.com/svn2github/word2vec/blob/master/questions-words.txt\n",
    "\n",
    "Q. When does it fail?\n",
    "\n",
    "Q. What are the possible reasons for failure?\n",
    "\n",
    "##### 15mins\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 11283,
     "status": "ok",
     "timestamp": 1737553132557,
     "user": {
      "displayName": "Eder Tarifa Fernandez",
      "userId": "12042907008888851310"
     },
     "user_tz": 0
    },
    "id": "xWVu38ePjh-i",
    "outputId": "b606035f-10c5-4edb-8537-f112e63363e2"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'london'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cosine_distance(u, v):\n",
    "    distance = 0.0\n",
    "    dot = np.dot(u,v)\n",
    "    norm_u = np.sqrt(np.sum(u**2))\n",
    "    norm_v = np.sqrt(np.sum(v**2))\n",
    "    distance = dot/(norm_u)/norm_v\n",
    "    return distance\n",
    "\n",
    "\n",
    "def find_analogy(word_a, word_b, word_c, word_vectors, word2index):\n",
    "    word_a = word_a.lower()\n",
    "    word_b = word_b.lower()\n",
    "    word_c = word_c.lower()\n",
    "\n",
    "    (e_a, e_b, e_c) = (word_vectors[word2index[word_a]],\n",
    "                       word_vectors[word2index[word_b]],\n",
    "                       word_vectors[word2index[word_c]])\n",
    "\n",
    "\n",
    "    max_cosine_sim = -999\n",
    "    best_word = None\n",
    "\n",
    "    for (w, i) in word2index.items():\n",
    "        if w in [word_a, word_b, word_c]:\n",
    "            continue\n",
    "        cosine_sim = cosine_distance(e_b - e_a, word_vectors[i] - e_c)\n",
    "\n",
    "        if cosine_sim > max_cosine_sim:\n",
    "            max_cosine_sim = cosine_sim\n",
    "            best_word = w\n",
    "\n",
    "    return best_word\n",
    "\n",
    "find_analogy('france', 'paris', 'england', wvecs, w2i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mqchbyIiCOcQ"
   },
   "source": [
    "# Advanced: Compositionality\n",
    "\n",
    "* Given access to only word representations, how can we build representations for phrases and sentences?\n",
    "\n",
    "  (Hint: algebraic operation is one way)\n",
    "\n",
    "\n",
    "* Compute the similarity score between two sentences on the STS.input.MSRpar.txt dataset from https://github.com/alvations/stasis/tree/master/STS-data/STS2012-train\n",
    "\n",
    "  (Please use 00-readme.txt in the corpus for details on the format)\n",
    "  \n",
    "* Measure the pearson correlation with the human scores in STS.gs.MSRpar.txt\n",
    "\n",
    "Q. What problems did you encounter when computing the scores?\n",
    "\n",
    "Q. What are alternative ways of computing the scores?\n",
    "\n",
    "Q. Using your composition method, compute representations for the following expressions and also list the top-5 most similar words:\n",
    "\n",
    "* New York\n",
    "* kick the bucket\n",
    "* post office\n",
    "\n",
    "  Does it work? What are the possible reasons?\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wk_GklVOSo4p"
   },
   "outputs": [],
   "source": [
    "txt = [line.strip() for line in urllib.request.urlopen('https://github.com/alvations/stasis/tree/master/STS-data/STS2012-train').readlines()]\n",
    "decoded_txt = [line.decode('utf-8') for line in txt]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qUwxb7CPt_n3"
   },
   "source": [
    "# References\n",
    "\n",
    "\n",
    "* [Distributed Representations of Words and Phrases and their Compositionality](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf): word2vec reference\n",
    "\n",
    "* [Eluciating the properties of semantic word representations](http://www.offconvex.org/2016/02/14/word-embeddings-1/): A global perspective\n",
    "\n",
    "* [Understanding the algebraic notions of semantic word representations](http://www.offconvex.org/2015/12/12/word-embeddings-1/): Why does the word-analogies task work with simple algebraic manipulations?\n",
    "\n",
    "* [Stemming And Lemmatization](https://www.datacamp.com/community/tutorials/stemming-lemmatization-python)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "https://github.com/ImperialNLP/NLPLabs-2025/blob/main/lab01-preprocessing-and-word-embeddings/lab01_PreprocessingAndEmbeddings.ipynb",
     "timestamp": 1737053099366
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
