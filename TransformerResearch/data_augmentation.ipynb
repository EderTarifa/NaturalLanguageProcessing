{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import tqdm as tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKING_ENV = 'local'  # Change as needed\n",
    "\n",
    "if WORKING_ENV == 'cluster':\n",
    "    content_path = '/vol/bitbucket/pvr24/nlp_cw_et1224_pvr24/'\n",
    "    data_path = f'{content_path}data'\n",
    "    cache_dir = f'{content_path}huggingface_cache'  # Define a cache directory on Bitbucket\n",
    "\n",
    "elif WORKING_ENV == 'local':\n",
    "    content_path = './'\n",
    "    data_path = './data/'\n",
    "    cache_dir = './huggingface_cache'\n",
    "    os.makedirs(data_path, exist_ok=True)\n",
    "\n",
    "else:\n",
    "    raise NotImplementedError()\n",
    "\n",
    "# Ensure cache directory exists\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "# Set Hugging Face cache directory\n",
    "os.environ[\"HF_HOME\"] = cache_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsv_path = data_path + '/dontpatronizeme_pcl.tsv'\n",
    "train_csv = data_path + '/train.csv'\n",
    "dev_official_csv = data_path + '/test.csv'\n",
    "\n",
    "\n",
    "train_df = pd.read_csv(train_csv)\n",
    "#dev_official_df = pd.read_csv(dev_official_csv)\n",
    "\n",
    "train_df = train_df.dropna(subset=['text'])\n",
    "#dev_official_df = dev_official_df.dropna(subset=['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from checkpoint: ./data//augmented_data_checkpoint_3900.csv\n",
      "Checkpoint saved at row 4000: ./data//augmented_data_checkpoint_4000.csv\n",
      "Checkpoint saved at row 4100: ./data//augmented_data_checkpoint_4100.csv\n",
      "Checkpoint saved at row 4200: ./data//augmented_data_checkpoint_4200.csv\n",
      "Checkpoint saved at row 4300: ./data//augmented_data_checkpoint_4300.csv\n",
      "Checkpoint saved at row 4400: ./data//augmented_data_checkpoint_4400.csv\n",
      "Checkpoint saved at row 4500: ./data//augmented_data_checkpoint_4500.csv\n",
      "Checkpoint saved at row 4600: ./data//augmented_data_checkpoint_4600.csv\n",
      "Checkpoint saved at row 4700: ./data//augmented_data_checkpoint_4700.csv\n",
      "Checkpoint saved at row 4800: ./data//augmented_data_checkpoint_4800.csv\n",
      "Checkpoint saved at row 4900: ./data//augmented_data_checkpoint_4900.csv\n",
      "Checkpoint saved at row 5000: ./data//augmented_data_checkpoint_5000.csv\n",
      "Checkpoint saved at row 5100: ./data//augmented_data_checkpoint_5100.csv\n",
      "Checkpoint saved at row 5200: ./data//augmented_data_checkpoint_5200.csv\n",
      "Checkpoint saved at row 5300: ./data//augmented_data_checkpoint_5300.csv\n",
      "Checkpoint saved at row 5400: ./data//augmented_data_checkpoint_5400.csv\n",
      "Checkpoint saved at row 5500: ./data//augmented_data_checkpoint_5500.csv\n",
      "Checkpoint saved at row 5600: ./data//augmented_data_checkpoint_5600.csv\n",
      "Checkpoint saved at row 5700: ./data//augmented_data_checkpoint_5700.csv\n",
      "Checkpoint saved at row 5800: ./data//augmented_data_checkpoint_5800.csv\n",
      "Checkpoint saved at row 5900: ./data//augmented_data_checkpoint_5900.csv\n",
      "Checkpoint saved at row 6000: ./data//augmented_data_checkpoint_6000.csv\n",
      "Checkpoint saved at row 6100: ./data//augmented_data_checkpoint_6100.csv\n",
      "Checkpoint saved at row 6200: ./data//augmented_data_checkpoint_6200.csv\n",
      "Checkpoint saved at row 6300: ./data//augmented_data_checkpoint_6300.csv\n",
      "Checkpoint saved at row 6400: ./data//augmented_data_checkpoint_6400.csv\n",
      "Checkpoint saved at row 6500: ./data//augmented_data_checkpoint_6500.csv\n",
      "Checkpoint saved at row 6600: ./data//augmented_data_checkpoint_6600.csv\n",
      "Checkpoint saved at row 6700: ./data//augmented_data_checkpoint_6700.csv\n",
      "Checkpoint saved at row 6800: ./data//augmented_data_checkpoint_6800.csv\n",
      "Checkpoint saved at row 6900: ./data//augmented_data_checkpoint_6900.csv\n",
      "Checkpoint saved at row 7000: ./data//augmented_data_checkpoint_7000.csv\n",
      "Checkpoint saved at row 7100: ./data//augmented_data_checkpoint_7100.csv\n",
      "Checkpoint saved at row 7200: ./data//augmented_data_checkpoint_7200.csv\n",
      "Checkpoint saved at row 7300: ./data//augmented_data_checkpoint_7300.csv\n",
      "Checkpoint saved at row 7400: ./data//augmented_data_checkpoint_7400.csv\n",
      "Error translating text: Dr Mayengbam Lalit Singh Recently honourable PM of India launched scheme called \" Doubling Farmers ' Income \" in order to uplift the welfare being of larger portion of Indians . The big questions felt by people are ( i ) How would it be possible in short period of time ? And ( ii ) is it an elusive doctrine to sweep vote bank of millions of farmers for upcoming parliamentary election in 2019 ? Across all over India ( except for a few states ) so many rigidities have to be solved in order to raise the real income of farmers . These rigidities are characterised by infrastructures , structures and institutions . The present article focuses on those rigidities only for Manipur which has been lagging behind many Indian states . Regarding infrastructure ( inputs ) , the state is found backward among those backward states of India . Introspecting water infrastructure , the state has been under-utilising water resources despite the abundant availability . For Kharif season crops farmers rely on rain water since the state is under the map of SE monsoon . However , during Rabi and Zayed seasons the state has been witnessing scarcity of water . In order to curb this shortage , government needs construction of multiple check dams and digging bore wells in those small streams flowing across the plain areas . During peak season , there is always a shortage of manures and pesticides due to landslide on national highways . Revolution of organic farming will lessen dependency on chemical fertilisers since the state has high potential of producing organic manures and pesticides . During post harvesting period , the state needs to construct cold storages so that perishable crops can be stored for agricultural off seasons . Regarding financial support to farming , banks are not integrated with farmers . Moreover state government does not organise crop insurance schemes in order to compensate the loss due to natural calamity . My previous article on \" Channelizing Mahatma Gandhi National Rural Employment Guarantee Act ( MGNREGA ) with Crop and Livestock Insurances in India \" postulates the model on how to compensate the loss . The second rigid character of farming is that there had been no structural change over decades . Farming in Manipur focuses on production of only coarse cereals which is characterised by immediate consumption . In this era of globalisation farming should have close introspection of demand and supply model of consumers across the world . Especially in hilly areas , cultivation of coarse cereal is just to meet local consumption and not for earning profits . Nowadays , farmers stated cultivation of illegal crops such as poppy illegally in order to earn huge profit . In order to prevent this , government should emphasise on cultivation cash crops such yongchak ( parkia speciosa ) , cardamom , tea , coffee , black pepper , etc. since these crops are highly demanded in international as well as domestic market . On the other hand this can reduce environmental destruction which is followed after short period zoom cultivation . In plain areas also main crops are also coarse cereals which are vulnerable to low profit . Farmers should diversify in cultivation of cash crops such as black rice , medicinal plants , oil seeds , mushrooms , etc . Moreover farmers should engage themselves in nonfarm sectors wherein both farm and nonfarm sectors can be fitted in supply and demand schedule for their inputs and outputs . One of the institutional rigidities faced by the farmers is asymmetric information on the technological development in agricultural sector . Krishi Vigyan Kendras ( KVKs ) had been instituted in every districts across India in order to fill the information gap between farmers and agricultural scientists . However their works had been found confined to laboratories only and their expertises are hardly in the knowledge domain of farmers . Government should frame policies for KVKs so that they should be available to farmers at least once in a week on alternative blocks . Another institutional rigidity we witness is financial inaccessibility of farmers to existing financial institutions ( banks ) . In India , banks are found to release funds to manufacturing and service sectors but not on agricultural sector . Too much official procedures and rigid financial securities create inaccessibility of farmers to banks . So the banks should be advised to make survey on the constructive environment wherein farmers should accede and return loans in time . Another important institutional rigidity is lack of marketing institutions such as marketing farms and agents which can deal in such agricultural products . Government should invite both local and nonlocal farms which can link farmers with consumers at price determined by market rate . Government also should have policies such as Minimum Support Price and construction of Cold Storages so that various types of agricultural products can be available during their offseason . The last most important institutional rigidities is inability of government to constitute an integral body which comprises above mentioned farmers body , financial institutes , KVKs , Marketing farms and government funded research institutes . If the integral body function with mutual exchange of workings with all institutional constituents , dream of \" Doubling Farmers ' Income \" can be achieved . Let this article may draw attention of Agricultural Ministry in fulfilling its ambition ., Error: the JSON object must be str, bytes or bytearray, not NoneType\n",
      "Checkpoint saved at row 7500: ./data//augmented_data_checkpoint_7500.csv\n",
      "Checkpoint saved at row 7600: ./data//augmented_data_checkpoint_7600.csv\n",
      "Checkpoint saved at row 7700: ./data//augmented_data_checkpoint_7700.csv\n",
      "Checkpoint saved at row 7800: ./data//augmented_data_checkpoint_7800.csv\n",
      "Checkpoint saved at row 7900: ./data//augmented_data_checkpoint_7900.csv\n",
      "Checkpoint saved at row 8000: ./data//augmented_data_checkpoint_8000.csv\n",
      "Error translating text: \" I 'm disabled , not unable ... BRO . \", Error: 'NoneType' object is not iterable\n",
      "Checkpoint saved at row 8100: ./data//augmented_data_checkpoint_8100.csv\n",
      "Error translating text: Who is a refugee ?, Error: 'NoneType' object is not iterable\n",
      "Checkpoint saved at row 8200: ./data//augmented_data_checkpoint_8200.csv\n",
      "Checkpoint saved at row 8300: ./data//augmented_data_checkpoint_8300.csv\n",
      "Checkpoint saved at row 8375: ./data//augmented_data_checkpoint_8375.csv\n",
      "Final augmented data saved to ./data//augmented_data.csv\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import os\n",
    "from googletrans import Translator  # Ensure this is installed and working\n",
    "\n",
    "translator = Translator()\n",
    "intermediate_langs = [\"fr\", \"es\", \"de\"]\n",
    "\n",
    "def back_translate_google(text):\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        print(f\"Skipping invalid text: {text}\")  # Debugging\n",
    "        return text  # Return unchanged\n",
    "    \n",
    "    try:\n",
    "        intermediate_lang = random.choice(intermediate_langs)\n",
    "        translated = translator.translate(text, src='en', dest=intermediate_lang).text\n",
    "        if translated is None:\n",
    "            print(f\"Translation failed for text: {text}\")\n",
    "            return text\n",
    "\n",
    "        back_translated = translator.translate(translated, src=intermediate_lang, dest='en').text\n",
    "        if back_translated is None:\n",
    "            print(f\"Back-translation failed for text: {text}\")\n",
    "            return text\n",
    "\n",
    "        return back_translated\n",
    "    except Exception as e:\n",
    "        print(f\"Error translating text: {text}, Error: {e}\")\n",
    "        return text  # Return original text if an error occurs\n",
    "\n",
    "# Define checkpoint number\n",
    "checkpoint_row = 3900\n",
    "checkpoint_path = data_path + f\"/augmented_data_checkpoint_{checkpoint_row}.csv\"\n",
    "\n",
    "# Load from checkpoint if exists, otherwise start fresh\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(f\"Loading from checkpoint: {checkpoint_path}\")\n",
    "    augmented_train_df = pd.read_csv(checkpoint_path)\n",
    "else:\n",
    "    print(\"No checkpoint found, starting from scratch.\")\n",
    "    augmented_train_df = train_df.copy()\n",
    "    augmented_train_df[\"translated_text\"] = \"\"\n",
    "\n",
    "# Resume processing from the checkpoint row\n",
    "for i in range(checkpoint_row, len(train_df)):  \n",
    "    text = train_df.loc[i, 'text']\n",
    "    augmented_train_df.loc[i, \"translated_text\"] = back_translate_google(text)\n",
    "    \n",
    "    # Save progress every 100 rows\n",
    "    if (i + 1) % 100 == 0 or i == len(train_df) - 1:\n",
    "        save_path = data_path + f\"/augmented_data_checkpoint_{i+1}.csv\"\n",
    "        augmented_train_df.to_csv(save_path, index=False)\n",
    "        print(f\"Checkpoint saved at row {i+1}: {save_path}\")\n",
    "\n",
    "# Final save\n",
    "final_save_path = data_path + \"/augmented_data.csv\"\n",
    "augmented_train_df.to_csv(final_save_path, index=False)\n",
    "print(f\"Final augmented data saved to {final_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\paulr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\paulr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "def get_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonym = lemma.name().replace('_', ' ')\n",
    "            if synonym.lower() != word.lower():  # Avoid identical replacements\n",
    "                synonyms.add(synonym)\n",
    "    return list(synonyms)\n",
    "\n",
    "def synonym_replace(sentence, ratio=0.1):\n",
    "    words = sentence.split()\n",
    "    eligible_words = [word for word in words if get_synonyms(word)]  # Words with synonyms\n",
    "    \n",
    "    if not eligible_words:\n",
    "        return sentence  # Return original if no words can be replaced\n",
    "    \n",
    "    n = max(1, int(len(words) * ratio))  # Determine number of words to replace based on ratio\n",
    "    words_to_replace = random.sample(eligible_words, min(n, len(eligible_words)))\n",
    "    \n",
    "    new_sentence = []\n",
    "    for word in words:\n",
    "        if word in words_to_replace:\n",
    "            new_sentence.append(random.choice(get_synonyms(word)))\n",
    "        else:\n",
    "            new_sentence.append(word)\n",
    "    \n",
    "    return ' '.join(new_sentence)\n",
    "\n",
    "\n",
    "\n",
    "synonym_train_df = train_df.copy()\n",
    "# Apply to train_df\n",
    "synonym_train_df[\"augmented_text\"] = synonym_train_df[\"text\"].apply(lambda x: synonym_replace(x, ratio=0.05))\n",
    "\n",
    "save_path = data_path + \"/synonym_augmented_data_005.csv\"\n",
    "synonym_train_df.to_csv(save_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
